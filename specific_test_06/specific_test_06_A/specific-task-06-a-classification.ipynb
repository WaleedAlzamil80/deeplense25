{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10923021,"sourceType":"datasetVersion","datasetId":6758858},{"sourceId":283715,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":243088,"modelId":264695}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import sys\nimport os\nfrom PIL import Image\nfrom glob import glob\nimport math\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom tqdm import tqdm\nfrom timm import create_model\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom sklearn.preprocessing import label_binarize\nfrom itertools import cycle\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-12T15:40:06.422988Z","iopub.execute_input":"2025-03-12T15:40:06.423384Z","iopub.status.idle":"2025-03-12T15:40:11.164349Z","shell.execute_reply.started":"2025-03-12T15:40:06.423352Z","shell.execute_reply":"2025-03-12T15:40:11.163343Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# DataLoader","metadata":{}},{"cell_type":"code","source":"class NPYClassificationDataset(Dataset):\n    def __init__(self, file_paths, labels, transform=None):\n        \"\"\"\n        Args:\n            file_paths (list): List of file paths to .npy files.\n            labels (list): Corresponding labels for classification.\n            transform (callable, optional): Optional transform to be applied on an image.\n        \"\"\"\n        self.file_paths = file_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.file_paths)\n\n    def __getitem__(self, idx):\n        label = self.labels[idx]\n\n        img = np.load(self.file_paths[idx], allow_pickle=True)\n        if label == 1:\n            img = img[0]\n        img = Image.fromarray(np.uint8(img * 255))\n\n        # Apply transformations if provided\n        if self.transform:\n            img = self.transform(img)\n\n        return img, torch.tensor(label, dtype=torch.long)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T15:40:11.165164Z","iopub.execute_input":"2025-03-12T15:40:11.165554Z","iopub.status.idle":"2025-03-12T15:40:11.171455Z","shell.execute_reply.started":"2025-03-12T15:40:11.165531Z","shell.execute_reply":"2025-03-12T15:40:11.170324Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"train_transforms = transforms.Compose([\n    # transforms.CenterCrop(100),\n    transforms.Resize(150, Image.LANCZOS),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomVerticalFlip(p=0.5),\n    transforms.RandomRotation(degrees=30),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])\n\nval_transforms = transforms.Compose([\n        transforms.Resize(150, Image.LANCZOS),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5], std=[0.5])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T15:40:11.173880Z","iopub.execute_input":"2025-03-12T15:40:11.174174Z","iopub.status.idle":"2025-03-12T15:40:11.194606Z","shell.execute_reply.started":"2025-03-12T15:40:11.174151Z","shell.execute_reply":"2025-03-12T15:40:11.193895Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"dataset_root = \"/kaggle/input/deeplense/SpecificTest_06_A/Dataset\"\naxion_files = sorted(glob(os.path.join(dataset_root, \"axion\", \"*.npy\")))\nno_sub_files = sorted(glob(os.path.join(dataset_root, \"no_sub\", \"*.npy\")))\ncdm_files = sorted(glob(os.path.join(dataset_root, \"cdm\", \"*.npy\")))\n\nall_files = no_sub_files + axion_files + cdm_files\nlabels = [0] * len(no_sub_files) + [1] * len(axion_files) + [2] * len(cdm_files)\n\n# First split: 90% train, 10% val (stratified)\ntrain_files, val_files, train_labels, val_labels = train_test_split(\n    all_files, labels, test_size=0.1, stratify=labels, random_state=42\n)\n\n# Train MAE only on no_sub_train_files\nbatch_size=512\ntrain_dataset = NPYClassificationDataset(train_files, train_labels, train_transforms)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=4, shuffle=True)\n\n# Validation set for MAE (later used in classification also)\nval_dataset = NPYClassificationDataset(val_files, val_labels, val_transforms)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T15:40:11.195991Z","iopub.execute_input":"2025-03-12T15:40:11.196232Z","iopub.status.idle":"2025-03-12T15:40:11.480770Z","shell.execute_reply.started":"2025-03-12T15:40:11.196209Z","shell.execute_reply":"2025-03-12T15:40:11.480023Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Model Architecture","metadata":{}},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T15:40:11.481635Z","iopub.execute_input":"2025-03-12T15:40:11.481971Z","iopub.status.idle":"2025-03-12T15:40:11.486014Z","shell.execute_reply.started":"2025-03-12T15:40:11.481938Z","shell.execute_reply":"2025-03-12T15:40:11.485139Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class EncoderViT(nn.Module):\n    def __init__(self, base=\"tiny\", p=0.25):\n        super(EncoderViT, self).__init__()\n\n        modelss = create_model(f\"vit_{base}_patch16_224\", pretrained=True)\n        modelss.patch_embed = nn.Identity()\n        modelss.head = nn.Identity() # now output shape is embed_dim (tiny: 192, base: 768)\n        self.set_dropout(modelss, p)\n\n        # self.model.pos_embed = nn.Identity()  # Bypass position encoding in timm\n        # Override `_pos_embed()` so ViT doesnâ€™t add its own position embedding\n        # def forward_pos_embed(x):\n        #     return x \n        # self.model._pos_embed = forward_pos_embed\n\n        self.encoder_blocks = modelss.blocks\n        self.norm = modelss.norm\n\n    def set_dropout(self, model, p):\n        \"\"\"Recursively set dropout probability in a model.\"\"\"\n        for name, module in model.named_modules():\n            if isinstance(module, nn.Dropout):\n                module.p = p\n\n    def forward(self, x):\n        for block in self.encoder_blocks:\n            x = block(x)\n\n        x = self.norm(x)\n\n        return x\n\nclass AttentionPooling(nn.Module):\n    def __init__(self, embed_dim):\n        super().__init__()\n        self.query = nn.Parameter(torch.randn(embed_dim))  # Learnable query vector\n        self.attn_weights = nn.Linear(embed_dim, 1)  # Linear layer to compute scores\n\n    def forward(self, x):\n        \"\"\"\n        x: (batch_size, num_patches, embed_dim)\n        Returns: (batch_size, embed_dim) - aggregated representation\n        \"\"\"\n        scores = self.attn_weights(x).squeeze(-1)  # (batch_size, num_patches)\n        attn_weights = torch.softmax(scores, dim=1)  # Normalize\n        pooled = torch.sum(attn_weights.unsqueeze(-1) * x, dim=1)\n        return pooled\n\n# Define the Encoder (ViT model)\nclass ClassifierViT(nn.Module):\n    def __init__(self, base=\"tiny\", input_dim=256, embed_dim=192, num_patches=196, p=0.25, num_classes=3):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.embedInput = nn.Linear(input_dim, self.embed_dim)\n        self.encoder = EncoderViT(base=base, p=p)\n        self.num_patches = num_patches\n        # Load saved components\n        checkpoint = torch.load(\"/kaggle/input/specific_task_06/pytorch/default/2/encoder_embedInput.pth\", weights_only=True)\n\n        self.embedInput.load_state_dict(checkpoint[\"embedInput\"])\n        self.encoder.load_state_dict(checkpoint[\"encoder\"])\n        self.attnetion_pool = AttentionPooling(embed_dim)\n        self.num_patches = num_patches\n        self.classifier = nn.Sequential(\n            nn.Linear(embed_dim, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(p=p),\n            nn.Linear(128, num_classes)\n        )\n\n        self.register_buffer(\"full_position_encoding\", self.sinusoidal_position_encoding(num_patches, self.embed_dim).unsqueeze(0))\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        \n        full_pos_encoding = self.full_position_encoding.expand(batch_size, -1, -1)\n        x = self.embedInput(x) + full_pos_encoding # (bs, visible_patches, embed_dim)\n        \n        x = self.encoder(x)\n        x = self.attnetion_pool(x)\n        x = self.classifier(x)\n        return x\n\n    def sinusoidal_position_encoding(self, num_patches, embed_dim):\n        position = torch.arange(num_patches).unsqueeze(1)  # Shape: (num_patches, 1)\n        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim))\n\n        pe = torch.zeros(num_patches, embed_dim)\n        pe[:, 1::2] = torch.sin(position * div_term)\n        pe[:, 0::2] = torch.cos(position * div_term)\n\n        return pe  # Shape: (num_patches, embed_dim)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T15:40:11.486759Z","iopub.execute_input":"2025-03-12T15:40:11.486990Z","iopub.status.idle":"2025-03-12T15:40:11.514214Z","shell.execute_reply.started":"2025-03-12T15:40:11.486971Z","shell.execute_reply":"2025-03-12T15:40:11.513346Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T15:40:11.515055Z","iopub.execute_input":"2025-03-12T15:40:11.515312Z","iopub.status.idle":"2025-03-12T15:40:11.538047Z","shell.execute_reply.started":"2025-03-12T15:40:11.515282Z","shell.execute_reply":"2025-03-12T15:40:11.537414Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def image_to_patches(img_tensor, patch_size=16):\n    _, C, H, W = img_tensor.shape\n    num_patches = (H // patch_size) * (W // patch_size)\n\n    # Split into patches\n    img_patches = img_tensor.unfold(2, patch_size, patch_size) # [1, 1, 224, 224] -> [1, 1, 14, 224, 16]\n    img_patches = img_patches.unfold(3, patch_size, patch_size)  # [1, 1, 14, 224, 16] -> [1, 1, 14, 14, 16, 16]\n    img_patches = img_patches.contiguous().view(-1, num_patches, patch_size * patch_size)  # Flatten patches [1, 1, 14, 14, 16, 16] -> [1, 196, 256]\n\n    return img_patches","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T15:40:11.538745Z","iopub.execute_input":"2025-03-12T15:40:11.538925Z","iopub.status.idle":"2025-03-12T15:40:11.555913Z","shell.execute_reply.started":"2025-03-12T15:40:11.538909Z","shell.execute_reply":"2025-03-12T15:40:11.555348Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"patch_size = 10\ninput_dim = patch_size**2\nnum_patches = int(150/patch_size)**2\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ClassifierViT(base=\"tiny\", embed_dim = 192, input_dim=input_dim, num_patches=num_patches)\nmodel = nn.DataParallel(model.to(device))\nprint(\"masked patches: \", int(0.75*225))\nprint(\"visible patches: \", num_patches - int(0.75*225))\nprint_trainable_parameters(model)\n\n# Optimizer & Loss\noptimizer = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-6) #  , weight_decay=2e-4 , weight_decay=1e-4\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\ncriterion = nn.CrossEntropyLoss()\n\n# Track metrics\ntrain_losses, val_losses = [], []\ntrain_accuracies, train_aucs = [], []\nval_accuracies, val_aucs = [], []\n\n# Training Loop\nnum_epochs = 100\nbest_val_auc = 0.0\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    total, correct = 0, 0\n    all_probs = []\n    all_labels = []\n\n    for images, batch_labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n        images = images.to(device)\n        batch_labels = batch_labels.to(device)\n\n        optimizer.zero_grad()\n        images = image_to_patches(images, patch_size)\n        outputs = model(images)\n        loss = criterion(outputs, batch_labels)\n\n        loss.backward()\n        optimizer.step()\n\n        # Statistics\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        correct += (predicted == batch_labels).sum().item()\n        total += batch_labels.size(0)\n        probs = torch.softmax(outputs, dim=1) # [:, 1]  # Probability for class 1\n        all_probs.extend(probs.cpu().detach().numpy())\n        all_labels.extend(batch_labels.cpu().numpy())\n\n\n    train_acc = correct / total\n    train_loss = running_loss / len(train_loader)\n\n    try:\n        train_auc = roc_auc_score(all_labels, all_probs, multi_class='ovr')\n    except ValueError:\n        train_auc = float('nan')\n\n    train_losses.append(train_loss)\n    train_accuracies.append(train_acc)\n    train_aucs.append(train_auc)\n\n    # Validation Step\n    model.eval()\n    val_correct, val_total = 0, 0\n    val_loss = 0.0\n    all_probs_test = []\n    all_labels_test = []\n\n    with torch.no_grad():\n        for batch_data, batch_labels in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n            batch_data = image_to_patches(batch_data, patch_size)\n\n            outputs = model(batch_data)\n            loss = criterion(outputs, batch_labels)\n\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            val_correct += (predicted == batch_labels).sum().item()\n            val_total += batch_labels.size(0)\n\n            # Collect probabilities and labels for ROC/AUC\n            probs = torch.softmax(outputs, dim=1) # [:, 1]  # Probability for class 1\n            all_probs_test.extend(probs.cpu().detach().numpy())\n            all_labels_test.extend(batch_labels.cpu().numpy())\n    \n    val_acc = val_correct / val_total\n    val_loss = val_loss / len(val_loader)\n    scheduler.step(val_loss)\n\n    try:\n        val_auc = roc_auc_score(all_labels_test, all_probs_test, multi_class='ovr')\n    except ValueError:\n        val_auc = float('nan') \n\n    val_losses.append(val_loss)\n    val_accuracies.append(val_acc)\n    val_aucs.append(val_auc)\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f},  Train AUC: {train_auc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val AUC: {val_auc:.4f}\")\n\n    # Save Best Model\n    if val_auc > best_val_auc:\n        best_val_auc = val_auc\n        torch.save(model.state_dict(), \"best_fine_tuned_vit_model_clas.pth\")\n        print(\"Model Saved (Best Validation AUC)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T15:40:11.556663Z","iopub.execute_input":"2025-03-12T15:40:11.556932Z"}},"outputs":[{"name":"stdout","text":"masked patches:  168\nvisible patches:  57\ntrainable params: 5383876 || all params: 5383876 || trainable%: 100.0\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/100:   1%|          | 1/157 [00:04<11:00,  4.23s/it]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)\n  return F.linear(input, self.weight, self.bias)\nEpoch 1/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [03:45<00:00,  1.44s/it]\nEpoch 1/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:09<00:00,  2.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/100] | Train Loss: 0.9760, Train Acc: 0.4739,  Train AUC: 0.6808 | Val Loss: 3.9772, Val Acc: 0.3976, Val AUC: 0.7928\nModel Saved (Best Validation AUC)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [03:48<00:00,  1.46s/it]\nEpoch 2/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:09<00:00,  1.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/100] | Train Loss: 0.7118, Train Acc: 0.6522,  Train AUC: 0.8295 | Val Loss: 0.9011, Val Acc: 0.5792, Val AUC: 0.9393\nModel Saved (Best Validation AUC)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [03:44<00:00,  1.43s/it]\nEpoch 3/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:09<00:00,  1.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/100] | Train Loss: 0.5121, Train Acc: 0.7849,  Train AUC: 0.9208 | Val Loss: 3.0710, Val Acc: 0.5419, Val AUC: 0.9100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/100:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 141/157 [03:22<00:22,  1.42s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## Plottings","metadata":{}},{"cell_type":"code","source":"sns.set(style=\"whitegrid\", font_scale=1.2)\nepochs = range(1, num_epochs + 1)\nsave_dir = \"/kaggle/working/\"\n\n# Plot Loss\nplt.figure(figsize=(10, 6))\nplt.plot(epochs, train_losses, label='Train Loss', color='blue')\nplt.plot(epochs, val_losses, label='Val Loss', color='red')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss over Epochs')\nplt.legend()\nplt.savefig(os.path.join(save_dir, 'Losses.png'))\nplt.show()\n\n# Plot Accuracy\nplt.figure(figsize=(10, 6))\nplt.plot(epochs, train_accuracies, label='Train Accuracy', color='blue')\nplt.plot(epochs, val_accuracies, label='Val Accuracy', color='red')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Accuracy over Epochs')\nplt.legend()\nplt.savefig(os.path.join(save_dir, 'Accuracies.png'))\nplt.show()\n\n# Plot AUC\nplt.figure(figsize=(10, 6))\nplt.plot(epochs, train_aucs, label='Train AUC', color='blue')\nplt.plot(epochs, val_aucs, label='Val AUC', color='red')\nplt.xlabel('Epoch')\nplt.ylabel('AUC')\nplt.title('AUC over Epochs')\nplt.legend()\nplt.savefig(os.path.join(save_dir, 'AUC.png'))\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"state_dict = torch.load(\"/kaggle/working/best_fine_tuned_vit_model_clas.pth\", map_location=device, weights_only=True)\nmodel.load_state_dict(state_dict)\n\nall_probs_test = []\nall_labels_test = []\nval_correct, val_total = 0, 0\n\nwith torch.no_grad():\n    for batch_data, batch_labels in val_loader:\n        batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n        batch_data = image_to_patches(batch_data, patch_size)\n\n        outputs = model(batch_data)\n\n        _, predicted = torch.max(outputs, 1)\n        val_correct += (predicted == batch_labels).sum().item()\n        val_total += batch_labels.size(0)\n\n        probs = torch.softmax(outputs, dim=1)\n        all_probs_test.extend(probs.cpu().detach().numpy())\n        all_labels_test.extend(batch_labels.cpu().numpy())\n\nval_acc = val_correct / val_total\nprint(f\"Accuracy: {(val_acc*100):.2f}%\")\n\n# Step 1: Binarize the labels\nall_labels_test = np.array(all_labels_test)\nall_probs_test = np.array(all_probs_test)\n\nn_classes = len(np.unique(all_labels_test))\nall_labels_test_bin = label_binarize(all_labels_test, classes=np.arange(n_classes))\n\n# Step 2: Compute ROC curve and AUC for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(all_labels_test_bin[:, i], all_probs_test[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Step 3: Compute micro-average ROC curve and AUC\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(all_labels_test_bin.ravel(), all_probs_test.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n# Step 4: Plot ROC curves\nlabel_map = {0:\"no\", 1:\"sphere\", 2: \"vort\"}\nplt.figure(figsize=(8, 6))\ncolors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\"])\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n             label=f\"ROC curve of class {label_map[i]} (AUC = {roc_auc[i]:.2f})\")\n\nplt.plot(fpr[\"micro\"], tpr[\"micro\"], color=\"deeppink\", linestyle=\":\", linewidth=4,\n         label=f\"Micro-average ROC curve (AUC = {roc_auc['micro']:.2f})\")\n\nplt.plot([0, 1], [0, 1], \"k--\", lw=2)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Multiclass ROC Curve\")\nplt.legend(loc=\"lower right\")\nplt.savefig(os.path.join(save_dir, 'ROC_curve.png'))\nplt.show()\n\n# Step 5: Compute AUC using roc_auc_score with multi_class='ovr'\nval_auc = roc_auc_score(all_labels_test, all_probs_test, multi_class='ovr')\nprint(f\"AUC (One-vs-Rest, macro-average): {val_auc:.2f}\")\n\n# Optional: Compute AUC with different averaging methods\nval_auc_micro = roc_auc_score(all_labels_test, all_probs_test, multi_class='ovr', average='micro')\nval_auc_weighted = roc_auc_score(all_labels_test, all_probs_test, multi_class='ovr', average='weighted')\nprint(f\"AUC (One-vs-Rest, micro-average): {val_auc_micro:.2f}\")\nprint(f\"AUC (One-vs-Rest, weighted-average): {val_auc_weighted:.2f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}